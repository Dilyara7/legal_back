# JUPYTER NOTEBOOK

############################################
# Установка необходимых библиотек
############################################
!pip install transformers datasets accelerate sentencepiece

import os
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments
from datasets import Dataset

############################################
# 1. Подготавливаем мини-датасет
# Формат: список словарей с полями "input" и "output"
############################################
mini_dataset = [
    {
        "input": "Какие документы нужны для регистрации ИП в Казахстане?",
        "output": "Для регистрации ИП в Казахстане необходимо подать заявление и удостоверение личности в регистрирующие органы, а также при необходимости оплатить сбор. Условия могут отличаться, если вид деятельности подлежит лицензированию."
    },
    {
        "input": "Какие требования к заключению договора аренды недвижимости по законам Республики Казахстан?",
        "output": "Договор аренды недвижимости в Казахстане заключается в письменной форме, должен содержать данные об объекте, сроке аренды и арендной плате. Подписи сторон обязательны, нотариальное удостоверение возможно, если того требует закон или стороны."
    },
    {
        "input": "Қанша жасқа толғанда кәмелетке толады және некеге тұруға болады?",
        "output": "Қазақстан Республикасында кәмелетке толу жасы 18 жас. Заңды некеге тұру да, жалпы жағдайда, 18 жастан рұқсат етіледі. Ерекше жағдайларда 16-17 жаста да некеге отыруға болады, егер заңды негіз болса."
    },
    {
        "input": "Могу ли я расторгнуть брак без присутствия супруга, если он уехал за границу?",
        "output": "Если супруг не выходит на связь и не даёт согласия, придётся обращаться в суд. Суд может рассмотреть дело без его личного присутствия, если он уведомлён и не возражает."
    },
    {
        "input": "Как правильно оформить завещание в Казахстане?",
        "output": "В Казахстане завещание должно быть составлено в письменной форме и удостоверено нотариусом. Завещатель вправе в любое время отменить или изменить его."
    }
]

# Преобразуем в Hugging Face Dataset
dataset = Dataset.from_list(mini_dataset)
# Для тренировки обычно делят на train/val, здесь упрощённо сделаем только train
dataset = dataset.train_test_split(test_size=0.2, seed=42)

train_dataset = dataset["train"]
eval_dataset = dataset["test"]

print("Train size:", len(train_dataset))
print("Eval size:", len(eval_dataset))

############################################
# 2. Загружаем модель mT5
############################################
model_name = "google/mt5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Опционально проверяем, есть ли GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

############################################
# 3. Подготовка функций для токенизации
############################################
MAX_SOURCE_LENGTH = 512
MAX_TARGET_LENGTH = 128

def preprocess_function(examples):
    # Берём поле "input" как исходный текст, поле "output" как целевой
    inputs = examples["input"]
    targets = examples["output"]
    
    model_inputs = tokenizer(
        inputs, max_length=MAX_SOURCE_LENGTH, truncation=True
    )
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            targets, max_length=MAX_TARGET_LENGTH, truncation=True
        )
    model_inputs["labels"] = labels["input_ids"]
    
    return model_inputs

train_dataset_tokenized = train_dataset.map(preprocess_function, batched=True)
eval_dataset_tokenized = eval_dataset.map(preprocess_function, batched=True)

############################################
# 4. Настраиваем Trainer
############################################
batch_size = 2  # для демонстрации (в реальности подбирайте по GPU)

training_args = Seq2SeqTrainingArguments(
    output_dir="./mt5-legal-finetuned",
    num_train_epochs=3,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    learning_rate=1e-4,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_dir="./logs",
    predict_with_generate=True,
    generation_max_length=MAX_TARGET_LENGTH,
    load_best_model_at_end=True
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset_tokenized,
    eval_dataset=eval_dataset_tokenized,
    tokenizer=tokenizer
)

############################################
# 5. Запуск обучения (fine-tuning)
############################################
trainer.train()

############################################
# 6. Пробуем сгенерировать ответ
# Проверим на примере из нашего датасета, но можно задать и новый вопрос
############################################
test_question = "Что делать, если работодатель в Казахстане задерживает зарплату?"
# Сформируем входные данные
inputs = tokenizer.encode(test_question, return_tensors="pt").to(device)

# Генерация
outputs = model.generate(
    inputs,
    max_length=MAX_TARGET_LENGTH,
    num_beams=4,
    early_stopping=True
)
answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("Вопрос:", test_question)
print("Ответ:", answer)
